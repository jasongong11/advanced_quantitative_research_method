---
title: "Lecture 3 Hypothesis Testing"
author: "Xuanjun Gong"
date: "2025-02-03"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Statistics

*"Statistics is a science of data collection analysis, and interpretation"*

2013 London workshop report on the Future of the Statistical Sciences

What is the difference between probability theory and statistics?

## Statistics

Difference between probability theory and statistics

-   In probability problems, we know that random mechanisms produced the data (flip a coin).
-   In statistics problem, the data comes from random mechanisms that is unknown, thus needs to be inferred.

We assume the random mechanism, and infer the parameters of the random process.

## Statistics Example

Flip a coin with $p=P(head)$

<img src="img/coin_toss.png" width="40%"/>

We can draw random samples from this distribution to estimate the parameter $p$.

## Statistics Example

What is the distribution for US adults' height?

We may assume a normal distribution $f(x)\sim N(\mu, \sigma^2)$

We draw random samples of US adults and estimate the parameter $\mu$ and $\sigma^2$.

In short, we build statistical models to interpret the data and infer how these data is being generated.

## Statistics

**Model**

A model is the distributions that generate the data.

$f(x|\theta)$ is called the model.

-   If $f$ is known, then this is a parametric model.
-   If $f$ is known, then this is a non-parametric model.

## Random Sample

Random samples is a set of random variables $\{X_1, X_2, X_3, ..., X_n\}$ of size $n$ from a population with probability function $f(x)$, from which the random variables are ***identically and independently*** drawn.

Here identically and independently are usually abbreviated as $i.i.d$.

-   Observing one sample $X_i$ does not impact another sample $X_j$.
-   All samples are drawn from the same population.

$$X_i \overset{\text{i.i.d}}{\sim} X
$$

## Random Sample Example

1.  Flip a coin
2.  Rolling a dice
3.  Drawing US adults samples from US adults population (loosely defined)

Why?

In fact, many of real data observations are approximate of random samples, because it is drawn without replacement. You are less likely to interview a person twice. Thus, it is not independent. We may consider they are independent when the size of the population is large enough, such as the US adult population.

## Probability function of random samples

The joint probability function of observing all of our random samples of size $n$ is the multiplication of all of their marginal probability.

$$P(X_1\cap X_2\cap ...\cap X_i) = \prod_{i=1}^{n} f(x_i|\theta)$$

**Why?**

## Sample mean

Now, we define a function of random samples $X_1, X_2, ..., X_n$ as the average of all random samples of size $n$.

$$\bar{X} = \frac{1}{n}\sum_{i=1}^{n}X_i$$

Given the random variable $X$ has mean $E[X] = \mu$, and variance $Var(X) = \sigma^2$, what is the expected value and the variance of sample mean $\bar{X}$?

## Sample mean

$$
\begin{align}
E[\bar{X}] & = E[\frac{1}{n}(X_1+X_2+X_3+...+X_n)] 
\\
& = \frac{1}{n}(E[X_1]+E[X_2]+E[X_3]+...+E[X_n])
\\
& = \frac{1}{n}(\mu+\mu+\mu+...+\mu)
\\
& = \frac{1}{n}n\mu
\\
& = \mu
\end{align}
$$

## Sample mean

$$
\begin{align}
Var(\bar{X}) & = Var(\frac{1}{n}\sum_{i=1}^{n}X_i)
\\
& = \frac{1}{n^2}Var(\sum_{i=1}^{n}X_i)
\\
& = \frac{1}{n^2}\sum_{i=1}^{n}Var(X_i) \ \ \ \ \ \ \ \ X_i\  \text{are i.i.d}
\\
& = \frac{1}{n^2}(n\sigma^2)
\\
& = \frac{\sigma^2}{n}
\end{align}
$$

## Statistics

In this case, $\bar{X}$ is a function of random samples $X_1, X_2, X_3, ..., X_n$.

We can use some functions of these random samples, such as $\bar{X}$, to infer the random variable $X$, where the random sample are drawn from.

We call these functions as ***Statistics***.

$$Y=T(X_1, X_2, ..., X_n)$$

Here $Y$ is a statistic, $T$ is the function.

$\bar{X}$ is a statistic of $X$.

## Sampling distribution

The probability distribution of statistic $Y$ is called sampling distribution of $Y$.

1.  **A statistic can be any function without involving parameter** $\theta$
2.  Parameter $\theta$ is for population, while statistics are for random samples.

We may use statistics to infer parameters.

## Statistic Example

We can use $\bar{X}$ to infer $\mu$.

Ex. What is the average height of US adults $\mu$?

Solution: We draw a random sample of US adults $X_1, X_2, ... X_n$, and calculate statistic $\bar{X}$, and use $\bar{X}$ to infer parameter $\mu$.

We call a estimator of a parameter **unbiased estimator** when the expected value of the estimator equal to the parameter.

Here $\bar{X}$ is an unbiased estimator of $\mu$.

## Statistics

In most cases, the parameter $\mu$ of the population can never be known.

Ex.

1.  How do we know if a coin is fair or not?
2.  How do we know the true average height for US adults?

## Sample variance

The next question is how do we know another parameter variance $\sigma^2$ of a random variable?

Take a guess?

$$Var(X) = E[(X-\mu)^2]$$

The most intuitive way to estimate variance is

$$
T(X_i) = \frac{1}{n}\sum(X_i-\mu)^2
$$

## Sample variance

But we can never know $\mu$, we only know $\bar{X}$

How about?

$$
T(X_i) = \frac{1}{n}\sum(X_i-\bar{X})^2
$$ This looks intuitively good.

But it is a unaccurate estimator for $\sigma^2$, because it is an biased estimator of $\sigma^2$.

## Sample variance

We define sample variance $S^2$ as

$$
S^2 = \frac{1}{n-1}\sum(X_i-\bar{X})^2
$$ Why is this better than our previous guess $\frac{1}{n}\sum(X_i-\bar{X})^2$?

## Sample variance

$$
\begin{align}
S^2 &= \frac{1}{n-1}\sum(X_i-\bar{X})^2
\\
&= \frac{1}{n-1}\sum(X_i^2-2\bar{X}X_i+\bar{X}^2)
\\
&= \frac{1}{n-1}(\sum X_i^2-2\bar{X}\sum X_i+\sum \bar{X}^2)
\\
&= \frac{1}{n-1}(\sum X_i^2-2\bar{X}n\bar{X}+n \bar{X}^2)
\\
&= \frac{1}{n-1}(\sum X_i^2-n\bar{X}^2)
\end{align}
$$

## Sample variance

$$
\begin{align}
E[S^2] &= E[\frac{1}{n-1}(\sum X_i^2-n\bar{X}^2)]
\\
&= \frac{1}{n-1}E[\sum X_i^2-n\bar{X}^2]
\\
&= \frac{1}{n-1}(E[\sum X_i^2]-E[n\bar{X}^2])
\\
&= \frac{1}{n-1}(nE[X_i^2]-nE[\bar{X}^2])
\\
&= \frac{n}{n-1}(E[X_i^2]-E[\bar{X}^2])
\end{align}
$$

## Sample variance

Given

$$
Var(X) = E[X^2] - E[X]^2
$$ We have

$$
E[X^2] = Var(X) + E[X]^2
$$

Thus

$$
E[X_i^2] = Var(X_i) + E[X_i]^2 = \sigma^2+\mu^2
\\
E[\bar{X}^2] = Var(\bar{X}) + E[\bar{X}]^2 = \frac{\sigma^2}{n}+\mu^2
$$

## Sample variance

$$
\begin{align}
E[S^2] & = \frac{n}{n-1}(E[X_i^2]-E[\bar{X}^2])
\\
& = \frac{n}{n-1}(\sigma^2+\mu^2-(\frac{\sigma^2}{n}+\mu^2))
\\
& = \frac{n}{n-1}(\sigma^2-\frac{\sigma^2}{n})
\\
& = \frac{n}{n-1}\frac{n-1}{n}\sigma^2
\\
& = \sigma^2
\end{align}
$$

From these derivations, we can learn that the sample variance $S^2=\sum(X_i-\bar{X})^2$ is an unbiased estimator of $\sigma^2$.

## Sample mean and sample variance

In summary, in order to know the random variable (the population), we need to use the statistics to infer its distribution.

We can draw random samples $X_i$, and use the sample mean $\bar{X}$ and sample variance $S^2$ as functions of the random samples to estimate the population parameter $\mu$, $\sigma^2$.

Why we are so interested in the population mean $\mu$, and the population variance $\sigma^2$?

## Sample mean and sample variance

$\bar{X}$ has a sampling distribution with expected value as $\mu$, and a variance as $\frac{\sigma^2}{n}$.

The more sample we have, the better our estimator $\bar{X}$ is, because the variance of $\bar{X}$ decreases as $n$ increases.

## Sampling from Normal distribution

From Central Limit Theorem, for any random variable X, and if the sample size $n$ is large enough, we would expect $\bar{X}\sim N(\mu, \frac{\sigma^2}{n})$.

What about if we do not have a large sample size $n$?

If we have two $i.i.d$ random sample from a normal distributed random variable, the sample mean will still be normally distributed.

This is because any linear combination of normal random variable is also a normal random variable.

## Sampling distribution of $\bar{X}$ and $S^2$

-   The sample mean of random samples from a normal random variable is also a normal random variable.

Thus we would expect

$$
\bar{X} \sim N(\mu, \frac{\sigma^2}{n})
$$

-   The sample variable $S^2$ has a expected value of $\sigma^2$, and $\frac{(n-1)S^2}{\sigma^2}$ has a distribution $\chi^x_{n-1}$.

$$
\frac{(n-1)S^2}{\sigma^2} \sim \chi_{n-1}^2
$$

-   $\bar{X}$ and $S^2$ are independent.

## $Z$ Statistic

Given that $\bar{X}$ follows a normal distribution.

We can standardize $\bar{X}$ as another statistic $Z$ defined as $\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}$.

$$
Z = \frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \sim N(0,1)
$$

## Sampling distribution of $\bar{X}$

However, we do not know $\sigma^2$, since it is a population parameter. This statistic $Z$ can not be computed without knowing $\sigma$.

What should we do?

## Sampling distribution of $\bar{X}$

The first intuition is to replace $\sigma$ with $S=\sqrt{S^2}$. This new statistic $\frac{\bar{X}-\mu}{S/\sqrt{n}}$ will not follow $Z$ distribution because $S$ is an estimator of $\sigma$.

What is the distribution of $\frac{\bar{X}-\mu}{S/\sqrt{n}}$?

## $t$ distribution

A $t$ distribution with degree of freedom $\nu$ is defined as follows

$$
t_{\nu} = \frac{Z}{\sqrt{V/\nu}}
\\
V\sim \chi^2_{\nu}
\\
Z, V \ \ \text{are independent}
$$

A $t$ distribution has the following $pdf$

$$
f(x) = \frac{1}{\sqrt{kx}}\frac{\Gamma(\frac{k+1}{2})}{\Gamma(\frac{k}{2})}(1+\frac{x^2}{k})^{-\frac{k+1}{2}}
\\
\Gamma(s)=\int_{0}^{\infty}t^{s-1}e^{-t}dt
$$

## $t$ distribution

```{r echo=FALSE}

x <- seq(-3, 3, length.out = 1000)

y1 <- dt(x, df=1)
y2 <- dt(x, df=2)
y3 <- dt(x, df=3)
y4 <- dt(x, df=5)
y5 <- dt(x, df=10)
y6 <- dt(x, df=100)
y7 <- dt(x, df=10000)

curve(dt(x, df=1), x, col = "blue", 
      ylab = "Density", xlab = "t value", 
      main = "t Distributions with Different Degrees of Freedom",
      xlim=c(-3,3), ylim=c(0, 0.5))
curve(dt(x, df=2), x, col = "red", add = TRUE)
curve(dt(x, df=3), x, col = "green", add = TRUE)
curve(dt(x, df=5), x, col = "orange", add = TRUE)
curve(dt(x, df=10), x, col = "pink", add = TRUE)
curve(dt(x, df=10), x, col = "purple", add = TRUE)
curve(dt(x, df=10000), x, col = "brown", add = TRUE)

legend("topright", legend = c("df=1", "df=2", "df=3", "df=5", "df=10", "df=100", "df=inf"),
       col = c("blue", "red", "green", "orange", "pink", "purple", "brown"), lty = 1)

```

## Properties of $t$ distribution

1.  $t$ distribution has one parameter as the degree of freedom $\nu$.
2.  $t$ distribution is unimodal, symmetric, and centered around 0.
3.  $t$ distribution is similar to $Z$ distribution but with flatter tails.
4.  When degree of freedom $\nu$ increases, the tail become less and less flat, until $\nu \rightarrow \infty$, then $t_{\nu} \rightarrow N(0,1)$

## $t$ distribution

It can be shown that the statistic $\frac{\bar{X}-\mu}{S/\sqrt{n}}$ follows a $t$ distribution with degree of freedom $n-1$.

$$
\frac{\bar{X}-\mu}{S/\sqrt{n}}\sim t_{n-1}
$$

We call this statistic $\frac{\bar{X}-\mu}{S/\sqrt{n}}$ as the $T$ statistic.

$$
T = \frac{\bar{X}-\mu}{S/\sqrt{n}}
$$

## t statistic derivation

$$
\begin{align}
\frac{\bar{X}-\mu}{S/\sqrt{N}} & = \frac{(\bar{X}-\mu)/(\sigma/\sqrt{n})}{(S/\sqrt{n})/(\sigma/\sqrt{n})}
\\
& = \frac{(\bar{X}-\mu)/(\sigma/\sqrt{n})}{S/\sigma}
\\
& = \frac{(\bar{X}-\mu)/(\sigma/\sqrt{n})}{\sqrt{\frac{S^2}{\sigma^2}}}
\\
& = \frac{(\bar{X}-\mu)/(\sigma/\sqrt{n})}{\sqrt{S^2/\sigma^2}}
\\
& = \frac{(\bar{X}-\mu)/(\sigma/\sqrt{n})}{\sqrt{\frac{(n-1)S^2/\sigma^2}{n-1}}}
\end{align}
$$

## t statistic derivation

Here

$$
(\bar{X}-\mu)/(\sigma/\sqrt{n}) \sim Z
$$

$$
(n-1)S^2/\sigma^2 \sim \chi_{n-1}^2
$$ Since

$$
t_{\nu} = \frac{Z}{\sqrt{V/\nu}}
\\
V\sim \chi^2_{\nu}
$$

Thus

$$
t = \frac{\bar{X}-\mu}{S/\sqrt{N}} \sim t_{n-1}
$$

## Summary

In summary, if $X\sim N(\mu, \sigma^2)$

1.  $\bar{X} \sim N(\mu, \frac{\sigma^2}{n})$
2.  $\frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \sim Z$
3.  $\frac{\bar{X}-\mu}{\S/\sqrt{n}} \sim t_{n-1}$

## F distribution

Previously, we have shown that the ratio of the two chi-squared distributed variables has a $F$ distribution. 

$$
\frac{U_1/p}{U_2/q} \sim F_{p,q}
\\
U_1 \sim \chi_p^2
\\
U_2 \sim \chi_q^2
$$

We also know that

$$
\frac{(n-1)S^2}{\sigma^2} \sim \chi_{n-1}^2
$$

## F distribution

We can use these two theorems to compare the variances of two populations. 

If we have $X_1, X_2, X_3, ... X_n$ as $n$ random samples drawn from random variable $X\sim N(\mu_x, \sigma^2_x)$, and $Y_1, Y_2, ..., Y_m$ as $m$ random samples drawn from random variable $Y \sim N(\mu_y, \sigma^2_y)$. 

Then the ratio between sample variances $\frac{S^2_x}{S^2_y}$ follows a $F$ distribution, if the two populations have the same variance $\sigma^2_x = \sigma^2_y$. 

## F distribution derivation
Under the assumption $\sigma^2_x = \sigma^2_y$, we will have $\frac{\sigma^2_x}{\sigma^2_y} = 1$

$$
\begin{align}
\frac{S^2_x}{S^2_y} & = \frac{S^2_x}{S^2_y}/\frac{\sigma^2_x}{\sigma^2_y}
\\
& = \frac{S^2_x}{\sigma^2_x}/\frac{S^2_y}{\sigma^2_y}
\\
& = \frac{(n-1)S^2_x}{(n-1)\sigma^2_x}/\frac{(m-1)S^2_y}{(m-1)\sigma^2_y}
\end{align}
$$

## F distribution derivation

Given that 

$$
U_1=\frac{(n-1)S^2_x}{\sigma^2_x} \sim \chi^2_{n-1}
\\
U_2=\frac{(m-1)S^2_y}{\sigma^2_y} \sim \chi^2_{m-1}
$$

Thus 

$$
\begin{align}
\frac{S^2_x}{S^2_y} & = \frac{(n-1)S^2_x}{(n-1)\sigma^2_x}/\frac{(m-1)S^2_y}{(m-1)\sigma^2_y}
\\
& = \frac{U_1/(n-1)}{U_2/(m-1)} \sim F_{n-1, m-1}
\end{align}
$$

## Hypothesis

What is a hypothesis?

A hypothesis is a statement about population parameter. 

Ex. 

$$
\mu=0
\\
p=0.5
\\
\frac{\sigma^2_x}{\sigma^2_y}=1
$$

## Hypothesis testing

We usually use two complementary hypothesis in a hypothesis testing problem. 

Null Hypothesis $H_0$ and alternative hypothesis $H_1$.\

Then, a hypothesis testing is a rule that specifies:

1. for which sample values, the decision is made to accept $H_0$
2. for which sample values $H_0$ is rejected and $H_1$ is accepted. 

We call the region of the sample value (statistics) that we will reject $H_0$ as the rejection region, or critical region, and the region we accept the $H_0$ as the acceptance region. 

## Hypothesis testing example

Ex. 

We want to test if a coin is biased.

Flip coin is a Bernoulli random process with probability of head $P(head) = p$. 

We specify 

$$
H_0: p = 0.5 \ \ \ \ \ \ \text{coin is not biased}
\\
H_0: p \neq 0.5 \ \ \ \ \ \ \text{coin is biased}
$$

## Hypothesis testing example

$H_0$ and $H_1$ are complementary.

If $H_0: p = 0.5$, then $H_1$ must be $H_1: p \neq0.5$

If $H_0: p \geq 0.5$, then $H_1$ must be $H_1: p < 0.5$

## Hypothesis testing example

Next, we can do experiment and draw random samples from this distribution. 

Suppose we flipped the coin 10 times. And we record the number of heads in the outcomes as $X$. Note $X$ here is a statistic of the random sample $X_1, X_2, X_3, ..., X_{10}$, as $X = \sum_{i=1}^{10}X_i$.

$X$ can take values ranging from 0 to 10. 

If we have observed $X=5$, what should we conclude?

If we have observed $X=6$, what should we conclude?

We need to find a region for $X$, if $X$ fall into that region, then we can make a decision. 

## Hypothesis testing example

What is the sampling distribution of $X$?

## Hypothesis testing example

$X$ follows a binomial distribution 

$$
X \sim Binomial(10, p)
\\
f(x) = {10\choose x} p^x(1-p)^{10-x}
$$

## Frequentist approach
To test the hypothesis, there are at least two schools of thoughts to address the problem. 

The first one is frequentist, and another is Bayesian. 

We will focus on frequentist approach, but will touch on Bayesian methods later. 

## Hypothesis testing with frequentist approach
We follow the following steps:

1. We assume $H_0$ is true.
2. We determine the distribution function of the statistic, assuming $H_0$ is true. 
3. We calculate the rejection region under significance level $\alpha$, in a way such that if the statistic fall into the rejection region, we reject $H_0$. 


## Hypothesis testing with frequentist approach
A equivalent and more conventional method is to calculate the $p$ value. 

1. We assume $H_0$ is true.
2. We calculate the probability of observing the statistic or more extreme values assuming $H_0$ is true. 
3. We set up a significance level $\alpha$, in a way such that if this probability $p < \alpha$, we reject $H_0$, other wise, we accept $H_0$. 

## Hypothesis testing example
Back to our case:

We assume $H_0: p = 0.5$ is true. Thus, we have the distribution of X.

## Hypothesis testing example

```{r}
x <- 0:10

prob <- dbinom(x,10,0.5)

plot(x, prob,
     type='h',
     xlab = "Number of success of tossing a fair coin twice",
        ylab = "Probability",
     main="Binomial distribution",
        ylim=c(0, 0.3))
```

## Hypothesis testing example

We can calculate the probability 

$P(X=0|p=0.5)=0.00098$, $P(X=1|p=0.5)=0.0098$, $P(X=2|p=0.5)=0.0439$, $P(X=3|p=0.5)=0.1172$, $P(X=4|p=0.5)=0.2051$, $P(X=5|p=0.5)=0.2461$, $P(X=6|p=0.5)=0.2051$, $P(X=7|p=0.5)=0.1172$, $P(X=8|p=0.5)=0.0439$, $P(X=9|p=0.5)=0.0098$, $P(X=10|p=0.5)=0.00098$


## Hypothesis testing example

Next, we calculate the rejection region for statistic $X$, in a way such that the probability that the observed statistic fall into this region will be less than the significance level $\alpha$. 

In this example, we start with $X=10$ and $X=0$, these values are the most extreme values in the possible outcomes. $P(X=10) + P(X=0) = 0.00196$. It means that the probability of observing a statistic belong to the region $X\in{0, 10}$ is 0.002, which is smaller than $\alpha$. 

## Hypothesis testing example

We then expand our rejection region to include $X=9$ and $X=1$, $P(X\in \{0,1,9,10\}) = 0.02156$, which is still less than $\alpha$.

Next, we calculate $P(X\in \{0,1,2, 8, 9,10\}) = 0.10936$, which is larger than significance level $\alpha$. 

Thus, our rejection region is $\{0,1, 9, 10\}$, because the probability that we observe statistic fall into this region is smaller than significance level $\alpha$. 

## Hypothesis testing example

```{r}
x <- 0:10

prob <- dbinom(x,10,0.5)

plot(x, prob,
     type='h',
     xlab = "Number of success of tossing a fair coin twice",
        ylab = "Probability",
     main="Binomial distribution",
        ylim=c(0, 0.3))

lines(c(0, 0), c(0, dbinom(0,10,0.5)), type='l', col="red")
lines(c(1, 1), c(0, dbinom(1,10,0.5)), type='l', col="red")
lines(c(9, 9), c(0, dbinom(9,10,0.5)), type='l', col="red")
lines(c(10, 10), c(0, dbinom(10,10,0.5)), type='l', col="red")
rect(xleft = -0.2, xright = 1.2, ybottom = -0.01, ytop = 0.03, border="red")
rect(xleft = 8.8, xright = 10.2, ybottom = -0.01, ytop = 0.03, border="red")
text(x=8.5, y=0.2, "P(X in region) <= 0.05")
```

## Hypothesis testing example

Suppose we observed $X=8$, what should we conclude?

Since $X \notin \{Rejection\}$, we do not reject $H_0$, and conclude that the coin is not biased. 

If we observe $X=9 \in \{Rejection\}$, we reject $H_0$, and conclude that the coin is biased.

## Hypothesis testing example

We can also calculate the $p$ value $X=8$.

$p$ value is defined as the probability of observing the statistic or more extreme values given $H_0$ is true.

$$p=P(X\in \{8,9,10,0,1,2\})=0.10936$$

This is larger than $\alpha$, thus we do not reject the null hypothesis


## Hypothesis testing example

In statistical programming, we do not need to do it by hand. The software can do it automatically. 

```{r echo=TRUE}
binom.test(x=8, n=10, p=0.5, alternative="two.sided", conf.level=0.05)
```


## Two-tailed vs. one-tailed

As you may have noticed, when we determine the rejection region and $p$ value, we are not only consider extreme values on left or right side, but consider both sides. 

The reason for this is because our $H_0$ is two-sided. 

$$
H_0: p = 0.5 \ \ \ \ \ \ \text{coin is not biased}
\\
H_0: p \neq 0.5 \ \ \ \ \ \ \text{coin is biased}
$$

Thus, the extreme value statistic against $H_0$ must include both large and small statistic on both sides of the distribution. 

## Two-tailed vs. one-tailed

What if we want to know if the coin is biased towards heads. 

$$
H_0: p \le 0.5 \ \ \ \ \ \ \text{coin is not biased towards head}
\\
H_0: p > 0.5 \ \ \ \ \ \ \text{coin is biased towards head}
$$

Now, it becomes a one-tailed test, where the extreme values only include large values not small values. Thus, we need to adjust the rejection region and $p$ values accordingly.

## Two-tailed vs. one-tailed

$$
P(X\in \{9,10\})=0.01078
\\
P(X\in \{8,9,10\})=0.05468
$$

Thus, in this case, our rejection region is $\{9, 10\}$. 

If $X < 9$, then we do not reject $H_0$ and conclude that the coin is not biased towards heads.
Other wise, we reject $H_0$, and conclude that the coin is biased towards heads.

## Two-tailed vs. one-tailed

```{r}
x <- 0:10

prob <- dbinom(x,10,0.5)

plot(x, prob,
     type='h',
     xlab = "Number of success of tossing a fair coin twice",
        ylab = "Probability",
     main="Binomial distribution",
        ylim=c(0, 0.3))

lines(c(9, 9), c(0, dbinom(9,10,0.5)), type='l', col="red")
lines(c(10, 10), c(0, dbinom(10,10,0.5)), type='l', col="red")
rect(xleft = 8.8, xright = 10.2, ybottom = -0.01, ytop = 0.03, border="red")
text(x=8.5, y=0.2, "P(Right tail) <= 0.05")
```

## Two-tailed vs. one-tailed

We can also calculate the $p$ value for one-tailed test. 

Suppose we have observed statistic $X=8$
$p=P(X\ge8 \ | \ p_{\theta}=0.5) = 0.05468$

## Two-tailed vs. one-tailed

```{r echo=TRUE}
binom.test(x=8, n=10, p=0.5, alternative="greater", conf.level=0.05)
```

## Two-tailed vs. one-tailed
Thus, if the sampling distribution for the statistic is symmetric (luckily, it is the case for most of our hypothesis testing problems).

$$
p_{one-tailed} = p_{two-tailed}/2
$$

And the hypothesis testing for two-tailed test with significance level $\alpha$ is equivalent to the one-tailed test with significance level $2\alpha$. Two-tailed test is more conservative.

## Z test
Suppose we want to test the US adults height $X \sim N(\mu, \sigma^2)$, with $\sigma^2 = 20$ with $\mu$ unknown. 

We want to test if Jason's height (172 cm) is taller or shorter than the average US height. We need to specify $H_0, H_1$

$$
H_0: \mu \le172
\\
H_1: \mu > 172
$$

## Z test

We collect $n$ random samples $X_1, X_2, X_3, ... X_n$ with $n=20$, and $\sigma^2 = 20$ known. The random samples $X_i$ follows the sample distributions as $X$, $X_i \overset{\text{i.i.d}}{\sim} X$. Thus, the sample mean follows the $\bar{X} \sim N(\mu, \frac{\sigma^2}{n})$.

By standardization, we know that 

$$
\frac{\bar{X}-\mu}{\frac{\sigma^2}{n}} \sim Z
\\
\sigma^2 = 20
$$


## Z test

Given that this is a one-tailed test. 

```{r}
x <- seq(-3, 3, length=100)
y <- dnorm(x, mean=0, sd=1)
x_range <- x[x>=1.644854]

plot(x, y, type = "l", lwd = 2, axes = FALSE, xlab = "x", ylab = "Probability Density",
     main="Z distribution")
axis(1, at = -2:4, labels = c("-2", "-1", "0", "1", "2", "3", "4"))
polygon(c(x_range[1], x_range, x_range[length(x_range)]), 
        c(0, dnorm(x_range, mean=0, sd=1), 0), 
        col = "pink") 
text(x=2, y=0.3, labels="P(Z>=z*) = 0.05", cex=1.5)

```


## Z test

Thus if $\frac{\bar{X}-\mu}{\sqrt{\sigma^2/n}} > z^*$, where $P(Z\ge z^*) =0.05$, we will have $p < 0.05$. 

Here 

$$p = P(Z \ge \frac{\bar{X}-\mu}{\sqrt{\sigma^2/n}} | \mu=172, \sigma^2=20, n=20)$$

## Z test rejection region

To compute the rejection region for $\bar{X}$, we need to find out $z^*$ where $P(Z\ge z^*) =0.05$. 

We can simply consult the Z tables or softwares. 

```{r echo=TRUE}
qnorm(0.95, mean=0, sd=1)
```

```{r echo=TRUE}
pnorm(1.644854, mean=0, sd=1)
```

## Z test rejection region

The rejection region needs to satify $\frac{\bar{X}-\mu}{\sqrt{\sigma^2/n}} > z^*$, where $z^*=1.645$.

Equivalently, $\bar{X}$ needs to satify $\bar{X} > \mu + z^*\sqrt{\sigma^2/n} $.

Thus the rejection region for $\bar{X}$ is $(\mu + z^*\sqrt{\sigma^2/n}, \infty)$.

Here 

$$
\mu + z^*\sqrt{\sigma^2/n}  = 172 + 1.645 * \sqrt{20/20}  = 173.645
$$

## Z test rejection region

In conclusion, we have calculated that the rejection region for $\bar{X}$ is $(173.645, \infty)$. 

If $\bar{X}$ is larger than 173.645, we reject the null hypothesis $H_0$, and accept $H_1$. 

Otherwise, we do not reject $H_0$, and accept $H_0$. 

If we have observed $\bar{X} = 175$, what should we conclude? 

## Z test p value

Another conventional method is to calculate the $p$ value. Here $p=P(Z\ge \frac{\bar{X}-\mu}{\sqrt{\sigma^2/n}}|\mu,\sigma^2, n)$.

We compute 

$$
\frac{\bar{X}-\mu}{\sqrt{\sigma^2/n}} = \frac{175-172}{\sqrt{20/20}} = 3
$$

## Z test p value

Thus $p=P(Z\ge 3)=1-P(Z<3)$, which can be found by 

```{r echo=TRUE}
1-pnorm(3, mean=0, sd=1)
```

Here we calculated $p=0.001 < 0.05$, thus we reject $H_0$ and accept $H_1$.

## Two-tailed Z test

Now, what if we want to conduct the two-tailed hypothesis testing?

We want to know if Jason's height (172 cm) is the US adult average height.

$$
H_0: \mu=172
\\
H_1: \mu\neq172
$$

What is our rejection region and $p$ value if observed $\bar{X} = 175$?

## Confidence interval method

Previously, we have discussed we can use the sample mean $\bar{X}$ as an estimator of $\mu$.

$$
\bar{X} = \hat{\mu}
$$

In this case, $\bar{X}$ is said as a point estimator of $\mu$, since it gives a singular value for $\mu$.

Another method to estimate $\mu$ is interval estimator, where we find a region $(L, U)$ that contains $\mu$ with a confidence interval $\gamma = 1-\alpha$. Usually, we set $\alpha=0.05$, and have $\gamma=95\%$.

## Confidence interval method

With this interval estimator $(L, U)$, we can say that we are $95\%$ confident that the parameter $\mu$ is contained in the interval estimator $(L, U)$. 

```{r}
x <- seq(-3, 3, length=100)
y <- dnorm(x, mean=0, sd=1)
x_range <- x[(x<=qnorm(0.975, mean=0, sd=1)) & (x>=qnorm(0.025, mean=0, sd=1))]

plot(x, y, type = "l", lwd = 2, axes = FALSE, xlab = "mu", ylab = "Probability Density",
     main="Distribution for mu")
axis(1, at = -3:3, labels = c("", "", "", "", "", "", ""))

polygon(c(x_range[1], x_range, x_range[length(x_range)]), 
        c(0, dnorm(x_range, mean=0, sd=1), 0), 
        col = "pink") 

text(x=0, y=0.05, labels="P(L<=mu<=U) = 0.95", cex=1.5)

```

## Confidence interval for $\mu$

How do we calculate this confidence interval $(L, U)$?

We know that 

$$
\frac{\bar{X}-\mu}{\sqrt{\sigma^2/n}} \sim Z
\\
P(z^*_- \le \frac{\bar{X}-\mu}{\sqrt{\sigma^2/n}} \le z^*_+) = 0.95
\\
P(Z\le z^*_+) = 1-\frac{\alpha}{2} =0.975
\\
P(Z\le z^*_-) = \frac{\alpha}{2} =0.025
$$


## Confidence interval for $\mu$

Equivalently, we have 

$$
P(\bar{X} + z^*_- \sqrt{\frac{\sigma^2}{n}} \le \mu \le \bar{X} + z^*_+ \sqrt{\frac{\sigma^2}{n}}) = 0.95
\\
z^*_+ = 1.96
\\
z^*_- = -1.96
$$

## Confidence interval for $\mu$

Thus the confidence interval with confidence level $\gamma = 1-\alpha=95\%$ is 

$(\bar{X} - 1.96 \sqrt{\frac{\sigma^2}{n}}, \bar{X} + 1.96 \sqrt{\frac{\sigma^2}{n}})$.

We plug in $\bar{X} = 175, \sigma^2=20, n=20$, we can calculate the confidence interval is $(173.04, 176.96)$.

This shows that we are $95\%$ confidence that the parameter $\mu$ is contained in the interval $(173.04, 176.96)$.

## Hypothesis testing with confidence interval

Importantly, the confidence interval is a two-tailed test as the confidence interval considers both sides of the distribution. 

Thus if we want to conduct a one-tailed test, we need to adjust the confidence level to $1-2\alpha=90\%$, and adjust the confidence interval accordingly. 

$$
\gamma=90\%
\\
z^*_+ = 1.64
\\
z^*_- = -1.64
$$

Thus, the confidence interval with $\gamma=90\%$ is (173.36,176.64).

## Hypothesis testing with confidence interval

Hypothesis testing with confidence interval is conducted to test if the confidence interval contains the null parameter set from the null hypothesis $\mu_0$. 

If the confidence interval $(L, U)$ contains $\mu_0$, then we do not reject $H_0$, otherwise we reject $H_0$, and accept $H_1$.

## Hypothesis testing with confidence interval

```{r}
x <- seq(-3, 3, length=100)
y <- dnorm(x, mean=0, sd=1)
x_range <- x[(x<=qnorm(0.95, mean=0, sd=1)) & (x>=qnorm(0.05, mean=0, sd=1))]

plot(x, y, type = "l", lwd = 2, axes = FALSE, xlab = "mu", ylab = "Probability Density",
     main="One-tailed test")
axis(1, at = -3:3, labels = c("172", "173", "174", "175", "176", "177", "178"))

polygon(c(x_range[1], x_range, x_range[length(x_range)]), 
        c(0, dnorm(x_range, mean=0, sd=1), 0), 
        col = "pink") 

text(x=0, y=0.05, labels="P(L<=mu<=U) = 0.9", cex=1.5)
abline(v=-3, col="red")
text(x=-2.5, y=0.1, labels="mu_null", cex=1.25)
```



## Hypothesis testing with confidence interval

```{r}
x <- seq(-3, 3, length=100)
y <- dnorm(x, mean=0, sd=1)
x_range <- x[(x<=qnorm(0.975, mean=0, sd=1)) & (x>=qnorm(0.025, mean=0, sd=1))]

plot(x, y, type = "l", lwd = 2, axes = FALSE, xlab = "mu", ylab = "Probability Density",
     main="Two-tailed test")
axis(1, at = -3:3, labels = c("172", "173", "174", "175", "176", "177", "178"))

polygon(c(x_range[1], x_range, x_range[length(x_range)]), 
        c(0, dnorm(x_range, mean=0, sd=1), 0), 
        col = "pink") 

text(x=0, y=0.05, labels="P(L<=mu<=U) = 0.95", cex=1.5)
abline(v=-3, col="red")
text(x=-2.5, y=0.1, labels="mu_null", cex=1.25)
```

## T test
In the above example, we assume $\sigma^2$ is known. However, $\sigma^2$ is not known in most real-world data cases, and we need to use sample variance $S^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X})^2$ to estimate $\sigma^2$. 

In this case, when $\sigma^2$ is known, and $\hat{\sigma^2} = S^2$, we will have a $t$ distribution.

$$
\frac{\bar{X}-\mu}{\sqrt{S^2/n}} \sim t_{n-1}
$$

## T test

Similar to Z test, we need to find the rejection region or $p$ value to test our hypothesis with significance level $\alpha$. 

Note that since when $n$ is extremely large, $t$ distribution will approximate a Z distribution. Thus, when $n$ is large, $t$ test will be equivalent to $Z$ test with $\sigma^2 \approx S^2$. 

How large is extremely large? Usually we consider when $n \ge 1000$ is extremely large, where you can not tell the difference. 

## T test for population mean

A professor believes that the students would study on average 10 hours per week for his course. To test his hypothesis, he sent out a survey to 10 randomly sampled students in his class asking how many hours per week they study on his course. The collected samples are

$$
11, 10, 14, 15, 5, 8, 9, 12, 6, 8
$$

Is the professor correct or not? 

## Example

To test this hypothesis with $p$ value, we need to follow the following steps:

1. State the null hypothesis and alternative hypothesis.
2. Calculate the $t$ statistic, and find the $t_{n-1}$ distribution.
3. Calculate the $p$ values based on the $t$ statistic and $t_{n-1}$ distribution.
4. Compare the $p$ value with significance level $\alpha$.
5. Reject null hypothesis if $p < \alpha$, otherwise accept null hypothesis. 


## State null hypothesis and alternative hypothesis

$$
H_0: \mu=10
\\
H_1: \mu \ne 10
$$

## Calculate $t$ statistic

$$
\bar{X} = \frac{1}{n}\sum{X_i} \ \ \ \ \ \ \ \bar{X} = 9.8
\\
S^2 = \frac{1}{n-1}\sum{(X_i-\bar{X})^2} \ \ \ \ \ \ S^2 = 10.622
\\
t = \frac{\bar{X} - \mu}{\sqrt{S^2/n}} \sim t_{n-1} \ \ \ \ \ \ t = -0.194
$$

## Calculate $p$ value

$$
p = P((t>t_+)\cup(t<t_-)|\mu=10) 
$$
```{r}
x <- seq(-3, 3, length=100)
y <- dt(x, df=10)
x_range_1 <- x[(x<=-0.194)]
x_range_2 <- x[(x>=0.194)]

plot(x, y, type = "l", lwd = 2, axes = FALSE, xlab = "t", ylab = "Probability Density",
     main="Two-tailed t test")
axis(1, at = -3:3, labels = c("-3", "-2", "-1", "0", "1", "2", "3"))

polygon(c(x_range_1[1], x_range_1, x_range_1[length(x_range_1)]), 
        c(0, dt(x_range_1, df=10), 0), 
        col = "pink") 

polygon(c(x_range_2[1], x_range_2, x_range_2[length(x_range_2)]), 
        c(0, dt(x_range_2, df=10), 0), 
        col = "pink") 

```

## Calculate $p$ value

```{r echo=TRUE}
pt(-0.194, df=9)*2 
```

Since $p = 0.850 > \alpha$, since we do not reject $H_0$, and conclude that the average student studying time for this course is 10 hours. 

## t.test with R code

```{r echo=TRUE}
study_time_samples <- c(11, 10, 14, 15, 5, 8, 9, 12, 6, 8)
t.test(study_time_samples, alternative="two.sided", mu=10)
```

## t test for mean comparison

In the previous example, we test the population mean for a single population $\mu = \mu_0$.

Sometimes, we have two populations or even more populations, and want to test if the mean for different populations are the same. 

## t test for mean comparison

Ex. 

- People's attitude of taking vaccine before and after viewing a promotional message. 
- The height different between males and females. 
- People's life expectancy time in experimental group (taking medicine) and control group (taking placebo). 

In general, t test mean comparison can be categorized into paired-sample and independent-sample mean comparison. 

## Paired-sample mean comparison

Paired-sample mean comparison can be conducted when the two group of random samples can be paired one to one. 

Suppose we have collected two observations from each individual in a group of random samples of size $n$. 

$$
X_1, X_2, X_3, ..., X_n
\\
Y_1, Y_2, Y_3, ..., Y_n
$$

We want to test the compare the mean difference between the two observations $X$ and $Y$ for $\mu_x$ and $\mu_y$. 

## Paired-sample mean comparison

We state the null hypothesis and alternative hypothesis

$$
H_0: \mu_x = \mu_y
\\
H_1: \mu_x \ne \mu_y
$$

## Paired-sample mean comparison

Since these two population are paired, we can calculate the difference between pairs of samples $\delta_i$.

Thus, we have a random sample of population differences $\delta_i$.

$$
\delta_i = X_i - Y_i
$$
And then we can conduct a one sample t test on $\delta_i$ with $\mu_{\delta} = 0$, since the null hypothesis state $\mu_x - \mu_y = 0$.

## Paired-sample mean example

Suppose we are interested in the effectiveness of a promotional message for promoting vaccines. 

<img src='img/vaccine_message.jpeg' width="40%">

## Paired-sample mean example

We randomly sampled 10 adults in our study, and measured their behavioral intention to take flu vaccine before viewing the message, where 1 encodes do not want to take vaccine at all, and 10 encodes want to take vaccine immediately.

$$
X_i = [2,3,5,4,2,1,5,7,3,5]
$$

We also measured their attitude after viewing the message for the same group of participants.

$$
Y_i = [3,4,5,6,2,9,9,6,4,6]
$$

## Paired-sample mean example

We want to know if the message is effective in promoting people's intention to take the vaccine.

What should we do?


## Paired-sample mean example

- Calculate paired-sample mean difference $\delta_i$
- State the null hypothesis and alternative hypothesis
- Calculate the sample mean $\bar{\delta}$ and sample variance $S^2_{\delta}$ for the mean differences $\delta_i$
- Calculate the $t$ statistic and determine its sampling distribution $t_{n-1}$
- Calculate the $p$ value and compare it with significance level $\alpha$
- Reject $H_0$ or not, and make conclusion


## Paired-sample mean example

We have 

$$
H0: \mu_{\delta} = 0
\\
H1: \mu_{\delta} \ne 0
\\
t = \frac{\bar{\delta}-\mu_{\delta}}{\sqrt{\sigma_{\delta}^2/n}} \sim t_{n-1}
\\
p = P((t>t_+)\cup(t<t_-))
$$

## Paired-sample mean with r code

```{r echo=TRUE}
intention_before <- c(2,3,5,4,2,1,5,7,3,5)
intention_after <- c(3,4,5,6,2,9,9,6,4,6)
t.test(intention_after, intention_before, alternative="two.sided",
       mu=0, paired=TRUE)
```

## Paired-sample mean with r code

```{r echo=TRUE}
t.test(intention_after-intention_before, alternative="two.sided",
       mu=0)
```

## Independent sample mean comparison

In some other cases, we do not have data that comes from same group of samples. 

When we want to test the mean difference between two groups of populations that are independently drawn, we need to use the independent sample mean comparison. 

## Example

Suppose we want to test the effectiveness differences between two types of promotional messages.

<img src='img/happy_appeal.jpg' width="40%">              <img src='img/fear_appeal.jpg' width="40%">

## Example

The study design is that we random sampled a group of people, and randomly separate them into two groups. 

We give the first group the first message and measured their behavioral intention $X_i = [5,7,9,6,10]$. 

Then give the second group the second message and measured their behavioral intention $Y_i = [2,3,5,7]$. 

## Independent sample mean comparison

We have two groups of random samples, one group of sample $X_i$ with sample size $n$ , and another group of sample $Y_i$ with sample size $m$. 

We can calculate the sample mean and sample variance for both groups independently $\bar{X}, S^2_x, \bar{Y}, S^2_y$. 

And we have our null hypothesis and alternative hypothesis. 

$$
H_0: \mu_x - \mu_y = 0
\\
H_1: \mu_x - \mu_y \ne 0
$$

## Independent sample mean comparison

When we assume the variance of the two population are the same $\sigma^2_x=\sigma^2_y$

We construct our $t$ statistic which follows a $t$ distribution with degree of freedom $n+m-2$

The idea is to estimate the sample variance with both samples as the pooled sample variance $S_p^2$. 

$$
t = \frac{\bar{X} - \bar{Y}}{\sqrt{S^2_p(\frac{1}{n}+\frac{1}{m})}} \sim t_{n+m-2}
\\
S_p^2 = \frac{\sum_{i=1}^n(X_i-\bar{X})^2 +\sum_{i=1}^m(Y_i-\bar{Y})^2}{m+n-2}
$$


## Independent-sample mean comparison example

$$
\bar{X} = \frac{1}{n}\sum X_i = 7.4
\\
\bar{Y} = \frac{1}{m}\sum Y_i = 4.25
\\
S_p^2 = \frac{\sum_{i=1}^n(X_i-\bar{X})^2 +\sum_{i=1}^m(Y_i-\bar{Y})^2}{m+n-2} = 4.564
\\
t = \frac{\bar{X} - \bar{Y}}{\sqrt{S^2_p(\frac{1}{n}+\frac{1}{m})}} = 2.198 \sim t_{n+m-2} = t_7
\\
p = P(t > t_+|\mu_{\delta=0}) + P(t < t_-|\mu_{\delta=0}) = 0.064
$$


## Independent-sample mean comparison with R
```{r echo=TRUE}
intention_group_happy <- c(5,7,9,6,10)
intention_group_fear <- c(2,3,5,7)
t.test(intention_group_happy, intention_group_fear, alternative="two.sided",
       mu=0, paired=FALSE, var.equal=TRUE)
```

## Independent sample without equal variance
In the case above, we assumed $\sigma^2_x=\sigma_y^2$, which is a pretty strong assumption to make especially when we have small sample sizes. 

When we do not assume $\sigma^2_x=\sigma_y^2$, the calculation will be a bit more complex. 

But the logic is the same, we need to construct hypothesis, find out $t$ statistic, find its $t$ distribution, and then calculate $p$ value. 

## Independent sample without equal variance

Here is how we construct our $t$ statistic.

$$
t = \frac{\bar{X}-\bar{Y}}{\sqrt{\frac{S^2_x}{n}+\frac{S^2_y}{m}}} \sim t_k
$$

Then the degree of freedom for the $t$ distribution $k$ is given

$$
k = \frac{(\frac{S^2_x}{n} + \frac{S^2_y}{m})^2}{\frac{1}{n-1}\frac{S^2_x}{n} + \frac{1}{m-1}\frac{S^2_y}{m}}
$$


## Independent-sample mean comparison with R

```{r echo=TRUE}
intention_group_happy <- c(5,7,9,6,10)
intention_group_fear <- c(2,3,5,7)
t.test(intention_group_happy, intention_group_fear, alternative="two.sided",
       mu=0, paired=FALSE, var.equal=FALSE)
```


## F test

F test is used to compare the variance of two **independent** variables. 

Recall that for two independent random variables $X$ and $Y$. 

If we collect $n$ samples $X_i$ from $X$, and $m$ samples of $Y_i$ from $Y$.

We would have 

$$
\frac{S^2_x/\sigma^2_x}{S^2_y/\sigma^2_y} \sim F_{n-1, m-1}
$$

## F test

In a F test, we set up the hypothesis

$$
H_0: \sigma_x^2 = \sigma_y^2
\\
H1: \sigma_x^2 \ne \sigma_y^2
$$

## F test

Thus under $H_0$, we will have 

$$
\sigma^2_x/\sigma_y^2 = 1
\\
\frac{S^2_x}{S^2_y} \sim F_{n-1, m-1}
$$

## F test

Thus, we can calculate the $F$ statistic and $p$ value, and test the hypothesis with $\alpha=0.05$ and $F_{n-1, m-1}$ distribution. 

$$
F' = \frac{S^2_x}{S^2_y} \sim F_{n-1, m-1}
\\
p = P(F>F')
$$

And if $p < \alpha$, we reject $H_0$, and accept $H_1$.

## F test

We may also calculate the $F$ statistic and then compare it with the critical value for the $F$ distribution $F^*$ 
$$
P(F>F^*) = \alpha=0.05
\\
F \sim F_{n-1, m-1}
$$
Then we compare the $F$ statistic with the critical value $F^*$.

if $F > F^*$, we reject $H_0$, otherwise, we accept $H_0$.

## F test

```{r}
x <- seq(0, 5, length=100)
y <- df(x, df1=10, df2=10)
x_range <- x[(x>=qf(0.95, 10, 10))]

plot(x, y, type = "l", lwd = 2, axes = FALSE, xlab = "t", ylab = "Probability Density",
     main="F test")
axis(1, at = 0:5, labels = c("0", "1", "2", "3", "4", "5"))

polygon(c(x_range[1], x_range, x_range[length(x_range)]), 
        c(0, df(x_range, 10, 10), 0), 
        col = "pink") 
text(x=3, y=0.2, labels="P(F>F*)=0.05", cex=1.25)
```
